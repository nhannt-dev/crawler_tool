# Crawler Tool API

H·ªá th·ªëng API tool crawler ƒë∆∞·ª£c x√¢y d·ª±ng v·ªõi ki·∫øn tr√∫c MVVM, s·ª≠ d·ª•ng NodeJS (TypeScript), Express, MySQL v√† Puppeteer.

## üìã M·ª•c l·ª•c

- [C√¥ng ngh·ªá s·ª≠ d·ª•ng](#-c√¥ng-ngh·ªá-s·ª≠-d·ª•ng)
- [Ki·∫øn tr√∫c h·ªá th·ªëng](#Ô∏è-ki·∫øn-tr√∫c-h·ªá-th·ªëng)
- [C√†i ƒë·∫∑t v√† ch·∫°y](#-c√†i-ƒë·∫∑t-v√†-ch·∫°y)
- [Database Schema](#Ô∏è-database-schema)
- [API Documentation](#-api-documentation)
  - [1. Initialize Crawl](#1-initialize-crawl)
  - [2. Modify Crawl](#2-modify-crawl)
  - [3. Purge Crawl](#3-purge-crawl)
  - [4. Get Sites List](#4-get-sites-list)
  - [5. Get Site Detail](#5-get-site-detail)
  - [6. Crawl and Create Category](#6-crawl-and-create-category)
- [Testing](#-testing)

---

## üöÄ C√¥ng ngh·ªá s·ª≠ d·ª•ng

| Technology | Version | Purpose |
|------------|---------|---------|
| **NodeJS** | 18+ | Runtime environment |
| **TypeScript** | 5.0+ | Type-safe JavaScript |
| **Express** | 4.18+ | Web framework |
| **MySQL** | 8.0+ | Relational database |
| **Puppeteer** | 21.6+ | Web scraping & automation |
| **Snowflake ID** | - | Distributed ID generation |
| **Slugify** | 1.6+ | URL-friendly slug generation |

---

## üèóÔ∏è Ki·∫øn tr√∫c h·ªá th·ªëng

H·ªá th·ªëng ƒë∆∞·ª£c x√¢y d·ª±ng theo **MVVM (Model-View-ViewModel) Pattern**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CLIENT REQUEST                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   ROUTES & VALIDATOR                 ‚îÇ
‚îÇ  - Route definition                                  ‚îÇ
‚îÇ  - Input validation                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CONTROLLER                        ‚îÇ
‚îÇ  - Nh·∫≠n request                                      ‚îÇ
‚îÇ  - Extract parameters                                ‚îÇ
‚îÇ  - G·ªçi Service                                       ‚îÇ
‚îÇ  - Format response                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                SERVICE (ViewModel)                   ‚îÇ
‚îÇ  - Business logic                                    ‚îÇ
‚îÇ  - Generate Snowflake ID                             ‚îÇ
‚îÇ  - Generate unique slug                              ‚îÇ
‚îÇ  - Puppeteer crawling                                ‚îÇ
‚îÇ  - Orchestrate repository calls                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    REPOSITORY                        ‚îÇ
‚îÇ  - Database operations (CRUD)                        ‚îÇ
‚îÇ  - Query execution                                   ‚îÇ
‚îÇ  - Data mapping to Models                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    MODEL                             ‚îÇ
‚îÇ  - Data structure & validation                       ‚îÇ
‚îÇ  - Entity representation                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                DATABASE (MySQL)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**C·∫•u tr√∫c th∆∞ m·ª•c:**

```
src/
‚îú‚îÄ‚îÄ config/           # Database connection pool
‚îú‚îÄ‚îÄ models/           # Entity models v·ªõi validation
‚îú‚îÄ‚îÄ repositories/     # Database operations (CRUD)
‚îú‚îÄ‚îÄ services/         # Business logic (ViewModel)
‚îú‚îÄ‚îÄ controllers/      # Request handlers
‚îú‚îÄ‚îÄ routes/           # API route definitions
‚îú‚îÄ‚îÄ validators/       # Input validation middleware
‚îú‚îÄ‚îÄ middlewares/      # Error handling, logging
‚îú‚îÄ‚îÄ types/            # TypeScript interfaces
‚îî‚îÄ‚îÄ utils/            # Helpers (Snowflake, Slugify)
```

---

## üîß C√†i ƒë·∫∑t v√† ch·∫°y

### 1. Prerequisites

- Node.js >= 18.0.0
- MySQL >= 8.0
- npm or yarn

### 2. C√†i ƒë·∫∑t dependencies

```bash
npm install
```

### 3. C·∫•u h√¨nh Database

T·∫°o file `.env` trong th∆∞ m·ª•c root:

```env
# Database
DB_HOST=localhost
DB_PORT=3306
DB_USER=root
DB_PASSWORD=your_password
DB_NAME=crawler_db

# Server
PORT=3000
NODE_ENV=development
```

### 4. T·∫°o Database

```bash
mysql -u root -p < database/schema.sql
```

### 5. Ch·∫°y Development Server

```bash
npm run dev
```

Server ch·∫°y t·∫°i: `http://localhost:3000`

### 6. Build Production

```bash
npm run build
npm start
```

---

## üóÑÔ∏è Database Schema

### Table: `crawl_site`

```sql
CREATE TABLE crawl_site (
  id VARCHAR(20) PRIMARY KEY,
  link_type ENUM('URL', 'API') NOT NULL,
  title VARCHAR(255) NOT NULL,
  crawl_link TEXT NOT NULL,
  slug VARCHAR(255) NOT NULL UNIQUE,
  status ENUM('INIT', 'RUNNING', 'DONE', 'ERROR') DEFAULT 'INIT',
  categories JSON NULL,
  sub_categories JSON NULL,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  
  INDEX idx_slug (slug),
  INDEX idx_status (status),
  INDEX idx_link_type (link_type),
  INDEX idx_created_at (created_at DESC)
);
```

### Table: `categories`

```sql
CREATE TABLE categories (
  id VARCHAR(20) PRIMARY KEY,
  site_id VARCHAR(20) NOT NULL,
  title VARCHAR(255) NOT NULL,
  slug VARCHAR(255) NOT NULL,
  title_selector VARCHAR(500) NOT NULL,
  link_selector VARCHAR(500) NOT NULL,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  
  FOREIGN KEY (site_id) REFERENCES crawl_site(id) ON DELETE CASCADE,
  UNIQUE KEY unique_slug_per_site (site_id, slug),
  INDEX idx_site_id (site_id),
  INDEX idx_slug (slug)
);
```

**Relationships:**

```
crawl_site (1) ‚îÄ‚îÄ‚îÄ (N) categories
    ‚îÇ
   id ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ site_id (ON DELETE CASCADE)
```

---

## üìö API Documentation

Base URL: `http://localhost:3000`

### 1. Initialize Crawl

T·∫°o m·ªõi m·ªôt task crawl site.

**Endpoint:** `POST /api/initialize-crawl`

**Request Body:**

```json
{
  "link_type": "URL",
  "title": "Example Website",
  "crawl_link": "https://example.com"
}
```

| Field | Type | Required | Description | Values |
|-------|------|----------|-------------|--------|
| link_type | string | Yes | Lo·∫°i link c·∫ßn crawl | "URL", "API" |
| title | string | Yes | Ti√™u ƒë·ªÅ (max 255 chars) | - |
| crawl_link | string | Yes | URL ho·∫∑c API endpoint | Valid URL |

**Success Response (201 Created):**

```json
{
  "success": true,
  "task_id": "1734512345678901234",
  "slug": "example-website-abc123",
  "status": "INIT",
  "message": "Crawl task initialized successfully"
}
```

**Error Response (400 Bad Request):**

```json
{
  "success": false,
  "error": "Validation Error",
  "message": "Title is required and must be a non-empty string.",
  "statusCode": 400
}
```

**cURL Example:**

```bash
curl -X POST http://localhost:3000/api/initialize-crawl \
  -H "Content-Type: application/json" \
  -d '{
    "link_type": "URL",
    "title": "Test Site",
    "crawl_link": "https://example.com"
  }'
```

---

### 2. Modify Crawl

C·∫≠p nh·∫≠t th√¥ng tin c·ªßa task crawl. H·ªó tr·ª£ **partial update** (ch·ªâ g·ª≠i field mu·ªën thay ƒë·ªïi).

**Endpoint:** `PATCH /api/modify-crawl/:site_id`

**URL Parameters:**

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| site_id | string | Yes | Task ID c·∫ßn update |

**Request Body (partial update):**

```json
{
  "title": "Updated Title"
}
```

Ho·∫∑c update nhi·ªÅu fields:

```json
{
  "link_type": "API",
  "title": "Updated Title",
  "crawl_link": "https://new-url.com"
}
```

| Field | Type | Required | Description | Values |
|-------|------|----------|-------------|--------|
| link_type | string | No | Lo·∫°i link c·∫ßn crawl | "URL", "API" |
| title | string | No | Ti√™u ƒë·ªÅ m·ªõi (max 255 chars) | - |
| crawl_link | string | No | URL ho·∫∑c API endpoint m·ªõi | Valid URL |

**L∆∞u √Ω:**
- H·ªó tr·ª£ **PARTIAL UPDATE** - ch·ªâ g·ª≠i field mu·ªën thay ƒë·ªïi
- **GI·ªÆ NGUY√äN ID** c·ªßa task
- Status reset v·ªÅ **INIT** khi update
- Categories/sub_categories reset v·ªÅ **null**
- N·∫øu update `title`, slug m·ªõi s·∫Ω t·ª± ƒë·ªông sinh
- Ph·∫£i g·ª≠i √≠t nh·∫•t 1 field

**Success Response (200 OK):**

```json
{
  "success": true,
  "task_id": "1734512345678901234",
  "message": "Crawl task modified successfully",
  "data": {
    "id": "1734512345678901234",
    "link_type": "URL",
    "title": "Updated Title",
    "crawl_link": "https://example.com",
    "slug": "updated-title-xyz123",
    "status": "INIT",
    "created_at": "2025-12-28T10:00:00.000Z"
  }
}
```

**cURL Example:**

```bash
# Update ch·ªâ title
curl -X PATCH http://localhost:3000/api/modify-crawl/1734512345678901234 \
  -H "Content-Type: application/json" \
  -d '{"title": "New Title"}'

# Update nhi·ªÅu fields
curl -X PATCH http://localhost:3000/api/modify-crawl/1734512345678901234 \
  -H "Content-Type: application/json" \
  -d '{
    "link_type": "API",
    "crawl_link": "https://api.new.com"
  }'
```

---

### 3. Purge Crawl

X√≥a vƒ©nh vi·ªÖn task crawl v√† t·∫•t c·∫£ d·ªØ li·ªáu li√™n quan.

**Endpoint:** `DELETE /api/purge-crawl/:site_id`

**URL Parameters:**

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| site_id | string | Yes | Task ID c·∫ßn x√≥a |

**L∆∞u √Ω:**
- **X√ìA Vƒ®NH VI·ªÑN** task kh·ªèi database
- Categories li√™n quan s·∫Ω b·ªã x√≥a theo (CASCADE DELETE)
- **Kh√¥ng th·ªÉ kh√¥i ph·ª•c** sau khi x√≥a

**Success Response (200 OK):**

```json
{
  "success": true,
  "deleted_id": "1734512345678901234"
}
```

**Error Response (404 Not Found):**

```json
{
  "success": false,
  "error": "Not Found",
  "message": "Crawl task not found.",
  "statusCode": 404
}
```

**cURL Example:**

```bash
curl -X DELETE http://localhost:3000/api/purge-crawl/1734512345678901234
```

---

### 4. Get Sites List

L·∫•y danh s√°ch sites v·ªõi filter v√† pagination.

**Endpoint:** `GET /api/sites`

**Query Parameters:**

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| page | number | No | 1 | Page number (1-indexed) |
| limit | number | No | 10 | Items per page (min: 1, max: 100) |
| filter | string | No | - | Filter by link_type ('URL' or 'API') |

**Success Response (200 OK):**

```json
{
  "data": [
    {
      "id": "1734512345678901234",
      "title": "Example Website",
      "slug": "example-website-abc123",
      "link_type": "URL",
      "status": "INIT",
      "created_at": "2025-12-28T10:00:00.000Z"
    }
  ],
  "pagination": {
    "page": 1,
    "limit": 10,
    "total": 25
  }
}
```

**cURL Examples:**

```bash
# Get all sites (default pagination)
curl http://localhost:3000/api/sites

# Get with custom pagination
curl "http://localhost:3000/api/sites?page=2&limit=20"

# Filter by URL type
curl "http://localhost:3000/api/sites?filter=URL"

# Combined filter + pagination
curl "http://localhost:3000/api/sites?filter=API&page=1&limit=5"
```

---

### 5. Get Site Detail

L·∫•y th√¥ng tin chi ti·∫øt site k√®m categories.

**Endpoint:** `GET /api/site/:slug`

**URL Parameters:**

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| slug | string | Yes | Site slug (URL-friendly ID) |

**Success Response (200 OK):**

```json
{
  "id": "1734512345678901234",
  "title": "Example Website",
  "slug": "example-website-abc123",
  "link_type": "URL",
  "status": "DONE",
  "categories": [
    {
      "id": "1734512345678901235",
      "title": "Technology",
      "slug": "technology-xyz",
      "title_selector": "h2.category-title",
      "link_selector": "a.category-link"
    }
  ]
}
```

**Error Response (404 Not Found):**

```json
{
  "success": false,
  "error": "Not Found",
  "message": "Site not found",
  "statusCode": 404
}
```

**cURL Example:**

```bash
curl http://localhost:3000/api/site/example-website-abc123
```

---

### 6. Crawl and Create Category

Crawl m·ªôt trang web ƒë·ªÉ tr√≠ch xu·∫•t **T·∫§T C·∫¢** categories matching selector v√† l∆∞u v√†o database. API t·ª± ƒë·ªông **b·ªè qua c√°c link trang ch·ªß/homepage**.

**Endpoint:** `POST /api/crawl/:site_id/category`

**URL Parameters:**

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| site_id | string | Yes | Site ID (Snowflake ID) |

**Request Body:**

```json
{
  "title_selector": "h2.category-title",
  "link_selector": "a.category-link"
}
```

| Field | Type | Required | Max Length | Description |
|-------|------|----------|------------|-------------|
| title_selector | string | Yes | 500 | CSS selector for category title |
| link_selector | string | Yes | 500 | CSS selector for category link |

**Important Notes:**
- API s·∫Ω crawl **T·∫§T C·∫¢ elements** matching selector
- **T·ª± ƒë·ªông b·ªè qua** c√°c element l√† home page/trang ch·ªß
- L∆∞u **batch** t·∫•t c·∫£ categories v√†o MySQL c√πng l√∫c
- C√°c keywords home page ƒë∆∞·ª£c filter: `home`, `homepage`, `trang ch·ªß`, `üè†`, etc.
- C√°c root path ƒë∆∞·ª£c filter: `/`, `#`, empty href

**Success Response (201 Created):**

```json
{
  "site_id": "1734512345678901234",
  "total_created": 5,
  "categories": [
    {
      "id": "1734512345678901235",
      "title": "Technology",
      "slug": "technology-abc123",
      "title_selector": "ul.menu-nav > li > a.nav-link",
      "link_selector": "ul.menu-nav > li > a.nav-link[href]"
    },
    {
      "id": "1734512345678901236",
      "title": "Sports",
      "slug": "sports-xyz456",
      "title_selector": "ul.menu-nav > li > a.nav-link",
      "link_selector": "ul.menu-nav > li > a.nav-link[href]"
    },
    {
      "id": "1734512345678901237",
      "title": "Entertainment",
      "slug": "entertainment-def789",
      "title_selector": "ul.menu-nav > li > a.nav-link",
      "link_selector": "ul.menu-nav > li > a.nav-link[href]"
    }
  ]
}
```

**Response Fields:**

| Field | Type | Description |
|-------|------|-------------|
| site_id | string | Site ID |
| total_created | number | S·ªë l∆∞·ª£ng categories ƒë√£ t·∫°o |
| categories | array | Danh s√°ch categories ƒë√£ crawl |
| categories[].id | string | Category ID (Snowflake) |
| categories[].title | string | Category title (extracted from page) |
| categories[].slug | string | URL-friendly slug (auto-generated) |
| categories[].title_selector | string | CSS selector used |
| categories[].link_selector | string | CSS selector used |

**Error Responses:**

**Site Not Found (404):**
```json
{
  "success": false,
  "error": "Not Found",
  "message": "Site not found",
  "statusCode": 404
}
```

**Validation Error (400):**
```json
{
  "success": false,
  "error": "Validation Error",
  "message": "title_selector is required; link_selector is required",
  "statusCode": 400
}
```

**Crawl Failed (500):**
```json
{
  "success": false,
  "error": "Internal Server Error",
  "message": "Crawl failed: No element found for selector: h2.category-title",
  "statusCode": 500
}
```

**No Valid Category (500):**
```json
{
  "success": false,
  "error": "Internal Server Error",
  "message": "No valid category found - all elements are home page links or empty",
  "statusCode": 500
}
```

**cURL Examples:**

```bash
# Crawl all categories from menu
curl -X POST http://localhost:3000/api/crawl/1734512345678901234/category \
  -H "Content-Type: application/json" \
  -d '{
    "title_selector": "ul.menu-nav > li > a.nav-link",
    "link_selector": "ul.menu-nav > li > a.nav-link[href]"
  }'

# Crawl specific section
curl -X POST http://localhost:3000/api/crawl/1734512345678901234/category \
  -H "Content-Type: application/json" \
  -d '{
    "title_selector": "div.sidebar h3.category-name",
    "link_selector": "div.sidebar a.category-url"
  }'
```

**Flow khi crawl nhi·ªÅu elements:**

```html
<!-- Example HTML -->
<ul class="menu-nav">
  <li><a class="nav-link" href="/">üè† Home</a></li>        <!-- SKIPPED -->
  <li><a class="nav-link" href="/">Trang ch·ªß</a></li>     <!-- SKIPPED -->
  <li><a class="nav-link" href="/tech">Technology</a></li> <!-- CREATED ‚úì -->
  <li><a class="nav-link" href="/sports">Sports</a></li>   <!-- CREATED ‚úì -->
  <li><a class="nav-link" href="/news">News</a></li>       <!-- CREATED ‚úì -->
</ul>
```

**K·∫øt qu·∫£:**
- Selector `ul.menu-nav > li > a.nav-link` match **5 elements**
- Skip **2 elements** ƒë·∫ßu (home page)
- Crawl **3 categories h·ª£p l·ªá**: Technology, Sports, News
- Batch insert v√†o MySQL
- Return `total_created: 3` v·ªõi array of 3 categories

---

## üß™ Testing

### Complete Workflow Test

```bash
#!/bin/bash

# 1. Create a site
echo "=== Creating site ==="
TASK_ID=$(curl -s -X POST http://localhost:3000/api/initialize-crawl \
  -H "Content-Type: application/json" \
  -d '{
    "link_type": "URL",
    "title": "Tech News",
    "crawl_link": "https://example.com"
  }' | jq -r '.task_id')

echo "Created site: $TASK_ID"

# 2. Get site list
echo -e "\n=== Getting sites list ==="
curl -s "http://localhost:3000/api/sites?limit=5" | jq

# 3. Create first category
echo -e "\n=== Creating first category ==="
curl -s -X POST http://localhost:3000/api/crawl/$TASK_ID/category \
  -H "Content-Type: application/json" \
  -d '{
    "title_selector": "ul.menu-nav > li > a.nav-link",
    "link_selector": "ul.menu-nav > li > a.nav-link[href]"
  }' | jq

# Response will contain ALL categories crawled (not just one)
# Example:
# {
#   "site_id": "...",
#   "total_created": 5,
#   "categories": [
#     {"id": "...", "title": "Technology", "slug": "technology-abc"},
#     {"id": "...", "title": "Sports", "slug": "sports-xyz"},
#     ...
#   ]
# }

# 4. Get site detail with categories
echo -e "\n=== Getting site detail ==="
SLUG=$(curl -s http://localhost:3000/api/sites | jq -r '.data[0].slug')
curl -s http://localhost:3000/api/site/$SLUG | jq

# 5. Modify site
echo -e "\n=== Modifying site ==="
curl -s -X PATCH http://localhost:3000/api/modify-crawl/$TASK_ID \
  -H "Content-Type: application/json" \
  -d '{"title": "Updated Tech News"}' | jq

# 6. Delete site (cascade delete categories)
echo -e "\n=== Deleting site ==="
curl -s -X DELETE http://localhost:3000/api/purge-crawl/$TASK_ID | jq

echo -e "\n=== Test completed ==="
```

Save script tr√™n v√†o `test.sh` v√† ch·∫°y:

```bash
chmod +x test.sh
./test.sh
```

---

## üìä API Summary

| Endpoint | Method | Purpose | Key Features |
|----------|--------|---------|--------------|
| `/api/initialize-crawl` | POST | T·∫°o task m·ªõi | Snowflake ID, unique slug |
| `/api/modify-crawl/:id` | PATCH | Update task | Partial update, ID preserved |
| `/api/purge-crawl/:id` | DELETE | X√≥a task | Cascade delete categories |
| `/api/sites` | GET | List sites | Filter, pagination |
| `/api/site/:slug` | GET | Site detail | Include categories |
| `/api/crawl/:id/category` | POST | Crawl categories | Puppeteer, auto skip home, batch insert |

---

## üéØ Best Practices

**MVVM Pattern:**
- ‚úÖ T√°ch bi·ªát concerns (Model, View, ViewModel)
- ‚úÖ Single Responsibility cho m·ªói layer
- ‚úÖ Business logic t·∫≠p trung ·ªü Service

**Security:**
- ‚úÖ Input validation v·ªõi middleware
- ‚úÖ Prepared statements (SQL injection prevention)
- ‚úÖ Environment variables cho sensitive data

**Performance:**
- ‚úÖ Database connection pooling
- ‚úÖ Indexes cho frequently queried columns
- ‚úÖ Pagination cho large datasets

**Error Handling:**
- ‚úÖ Centralized error handler
- ‚úÖ Meaningful error messages
- ‚úÖ Proper HTTP status codes

---

## üìñ Snowflake ID

Snowflake ID l√† thu·∫≠t to√°n sinh ID ph√¢n t√°n c·ªßa Twitter:

**Structure (64 bits):**
```
[41 bits: Timestamp] [5 bits: Datacenter] [5 bits: Worker] [12 bits: Sequence]
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Unique trong h·ªá th·ªëng ph√¢n t√°n
- ‚úÖ Time-ordered (ID tƒÉng theo th·ªùi gian)
- ‚úÖ High performance (h√†ng ngh√¨n ID/gi√¢y)
- ‚úÖ No central coordination needed

**Example:**
```
ID: 1734512345678901234
Timestamp: 2025-12-28 10:30:45
Worker: 1
Sequence: 789
```

---

## üìÑ License

MIT License

---

## üë®‚Äçüíª Author

Backend Architect - Senior Level
