# Crawler Tool API

H·ªá th·ªëng API tool crawler ƒë∆∞·ª£c x√¢y d·ª±ng v·ªõi ki·∫øn tr√∫c MVVM, s·ª≠ d·ª•ng NodeJS (TypeScript), Express, MySQL v√† Puppeteer.

## üìã M·ª•c l·ª•c

- [C√¥ng ngh·ªá s·ª≠ d·ª•ng](#c√¥ng-ngh·ªá-s·ª≠-d·ª•ng)
- [Ki·∫øn tr√∫c h·ªá th·ªëng](#ki·∫øn-tr√∫c-h·ªá-th·ªëng)
- [C·∫•u tr√∫c th∆∞ m·ª•c](#c·∫•u-tr√∫c-th∆∞-m·ª•c)
- [C√†i ƒë·∫∑t v√† ch·∫°y](#c√†i-ƒë·∫∑t-v√†-ch·∫°y)
- [Database Schema](#database-schema)
- [API Documentation](#api-documentation)
  - [1. Initialize Crawl](#1-initialize-crawl)
  - [2. Modify Crawl](#2-modify-crawl)
  - [3. Purge Crawl](#3-purge-crawl)
  - [4. Get Sites List](#4-get-sites-list)
  - [5. Get Site Detail](#5-get-site-detail)
  - [6. Crawl and Create Category](#6-crawl-and-create-category)
- [Lu·ªìng x·ª≠ l√Ω](#lu·ªìng-x·ª≠-l√Ω)
- [Testing](#testing)

## üöÄ C√¥ng ngh·ªá s·ª≠ d·ª•ng

- **NodeJS** v·ªõi **TypeScript** - Type-safe JavaScript
- **Express** - Web framework
- **MySQL** - Relational database
- **Puppeteer** - Web scraping v√† automation
- **Snowflake ID** - Distributed ID generation
- **Slugify** - URL-friendly slug generation

## üèóÔ∏è Ki·∫øn tr√∫c h·ªá th·ªëng

H·ªá th·ªëng ƒë∆∞·ª£c x√¢y d·ª±ng theo **MVVM (Model-View-ViewModel) Pattern**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CLIENT REQUEST                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CONTROLLER                        ‚îÇ
‚îÇ  - Nh·∫≠n request                                      ‚îÇ
‚îÇ  - Validate input (Validator)                        ‚îÇ
‚îÇ  - G·ªçi Service                                       ‚îÇ
‚îÇ  - Format response                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SERVICE (ViewModel)               ‚îÇ
‚îÇ  - Business logic                                    ‚îÇ
‚îÇ  - Generate Snowflake ID                             ‚îÇ
‚îÇ  - Generate unique slug                              ‚îÇ
‚îÇ  - Orchestrate repository calls                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    REPOSITORY                        ‚îÇ
‚îÇ  - Database operations (CRUD)                        ‚îÇ
‚îÇ  - Query execution                                   ‚îÇ
‚îÇ  - Data mapping                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    MODEL                             ‚îÇ
‚îÇ  - Data structure                                    ‚îÇ
‚îÇ  - Business validation                               ‚îÇ
‚îÇ  - Entity representation                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DATABASE (MySQL)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### T·∫°i sao ch·ªçn MVVM?

‚úÖ **Separation of Concerns**: M·ªói layer c√≥ tr√°ch nhi·ªám ri√™ng bi·ªát  
‚úÖ **Testability**: D·ªÖ d√†ng unit test t·ª´ng layer ƒë·ªôc l·∫≠p  
‚úÖ **Scalability**: D·ªÖ m·ªü r·ªông v√† th√™m features m·ªõi  
‚úÖ **Maintainability**: Code r√µ r√†ng, d·ªÖ b·∫£o tr√¨  
‚úÖ **Reusability**: Service v√† Repository c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng  

## üìÅ C·∫•u tr√∫c th∆∞ m·ª•c

```
crawler_tool/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database.ts              # MySQL connection pool config
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CrawlSite.model.ts       # CrawlSite entity model
‚îÇ   ‚îú‚îÄ‚îÄ repositories/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CrawlSite.repository.ts  # Database operations
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CrawlSite.service.ts     # Business logic (ViewModel)
‚îÇ   ‚îú‚îÄ‚îÄ controllers/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CrawlSite.controller.ts  # Request handlers
‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ crawl.routes.ts          # API routes definition
‚îÇ   ‚îú‚îÄ‚îÄ middlewares/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ errorHandler.ts          # Global error handling
‚îÇ   ‚îú‚îÄ‚îÄ validators/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CrawlSite.validator.ts   # Request validation
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ snowflake.ts             # Snowflake ID generator
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ slugify.ts               # Slug generator
‚îÇ   ‚îú‚îÄ‚îÄ types/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.ts                 # TypeScript interfaces & enums
‚îÇ   ‚îî‚îÄ‚îÄ app.ts                       # Express app entry point
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îî‚îÄ‚îÄ schema.sql                   # Database schema
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ tsconfig.json
‚îú‚îÄ‚îÄ .env.example
‚îî‚îÄ‚îÄ README.md
```

## üîß C√†i ƒë·∫∑t v√† ch·∫°y

### 1. Prerequisites

- Node.js >= 18.x
- MySQL >= 8.0
- npm ho·∫∑c yarn

### 2. C√†i ƒë·∫∑t dependencies

```bash
npm install
```

### 3. C·∫•u h√¨nh Database

T·∫°o file `.env` t·ª´ `.env.example`:

```bash
cp .env.example .env
```

C·∫≠p nh·∫≠t th√¥ng tin database trong `.env`:

```env
PORT=3000
NODE_ENV=development

DB_HOST=localhost
DB_PORT=3306
DB_USER=root
DB_PASSWORD=your_password
DB_NAME=crawler_db

DATACENTER_ID=1
WORKER_ID=1
```

### 4. T·∫°o Database

Ch·∫°y SQL script ƒë·ªÉ t·∫°o database v√† table:

```bash
mysql -u root -p < database/schema.sql
```

Ho·∫∑c import tr·ª±c ti·∫øp v√†o MySQL:

```sql
source database/schema.sql;
```

### 5. Ch·∫°y Development Server

```bash
npm run dev
```

Server s·∫Ω ch·∫°y t·∫°i: `http://localhost:3000`

### 6. Build Production

```bash
npm run build
npm start
```

## üìö API Documentation

### Base URL

```
http://localhost:3000/api
```

### Health Check

```http
GET /health
```

**Response:**

```json
{
  "success": true,
  "message": "Server is running",
  "timestamp": "2025-12-28T10:00:00.000Z"
}
```

---

### 1. Initialize Crawl Task

T·∫°o m·ªôt task crawl m·ªõi.

**Endpoint:** `POST /api/initialize-crawl`

**Request Body:**

```json
{
  "link_type": "URL",
  "title": "Crawl Example Website",
  "crawl_link": "https://example.com"
}
```

**Request Fields:**

| Field       | Type   | Required | Description                    | Values      |
| ----------- | ------ | -------- | ------------------------------ | ----------- |
| link_type   | string | Yes      | Lo·∫°i link c·∫ßn crawl            | "URL", "API"|
| title       | string | Yes      | Ti√™u ƒë·ªÅ task (max 255 chars)   |             |
| crawl_link  | string | Yes      | URL ho·∫∑c API endpoint          |             |

**Success Response (201 Created):**

```json
{
  "success": true,
  "task_id": "1734512345678901234",
  "message": "Crawl task initialized successfully",
  "data": {
    "id": "1734512345678901234",
    "link_type": "URL",
    "title": "Crawl Example Website",
    "crawl_link": "https://example.com",
    "slug": "crawl-example-website-lm3k9x",
    "status": "INIT",
    "created_at": "2025-12-28T10:00:00.000Z"
  }
}
```

**Error Response (400 Bad Request):**

```json
{
  "success": false,
  "error": "Bad Request",
  "message": "link_type must be either \"URL\" or \"API\"",
  "statusCode": 400
}
```

**Error Response (409 Conflict):**

```json
{
  "success": false,
  "error": "Conflict",
  "message": "Slug already exists. Please use a different title.",
  "statusCode": 409
}
```

---

### 2. Get Crawl Task by ID

L·∫•y th√¥ng tin task theo ID.

**Endpoint:** `GET /api/crawl/:id`

**Example:**

```http
GET /api/crawl/1734512345678901234
```

**Success Response (200 OK):**

```json
{
  "success": true,
  "data": {
    "id": "1734512345678901234",
    "link_type": "URL",
    "title": "Crawl Example Website",
    "crawl_link": "https://example.com",
    "slug": "crawl-example-website-lm3k9x",
    "status": "INIT",
    "categories": null,
    "sub_categories": null,
    "created_at": "2025-12-28T10:00:00.000Z",
    "updated_at": "2025-12-28T10:00:00.000Z"
  }
}
```

**Error Response (404 Not Found):**

```json
{
  "success": false,
  "error": "Not Found",
  "message": "Crawl task not found",
  "statusCode": 404
}
```

---

### 3. Get Crawl Task by Slug

L·∫•y th√¥ng tin task theo slug.

**Endpoint:** `GET /api/crawl/slug/:slug`

**Example:**

```http
GET /api/crawl/slug/crawl-example-website-lm3k9x
```

**Response:** Gi·ªëng nh∆∞ API Get by ID

---

### 4. Get All Crawl Tasks

L·∫•y danh s√°ch t·∫•t c·∫£ tasks v·ªõi pagination.

**Endpoint:** `GET /api/crawls`

**Query Parameters:**

| Parameter | Type   | Default | Description           |
| --------- | ------ | ------- | --------------------- |
| page      | number | 1       | Page number           |
| pageSize  | number | 10      | Items per page        |

**Example:**

```http
GET /api/crawls?page=1&pageSize=10
```

**Success Response (200 OK):**

```json
{
  "success": true,
  "data": [
    {
      "id": "1734512345678901234",
      "link_type": "URL",
      "title": "Crawl Example Website",
      "crawl_link": "https://example.com",
      "slug": "crawl-example-website-lm3k9x",
      "status": "INIT",
      "categories": null,
      "sub_categories": null,
      "created_at": "2025-12-28T10:00:00.000Z",
      "updated_at": "2025-12-28T10:00:00.000Z"
    }
  ],
  "pagination": {
    "page": 1,
    "pageSize": 10,
    "total": 1
  }
}
```

---

### 5. Modify Crawl Task

C·∫≠p nh·∫≠t th√¥ng tin c·ªßa m·ªôt task crawl ƒë√£ t·ªìn t·∫°i. API n√†y h·ªó tr·ª£ **partial update** - ch·ªâ c·∫ßn g·ª≠i c√°c field mu·ªën thay ƒë·ªïi.

**Endpoint:** `PATCH /api/modify-crawl/:site_id`

**URL Parameters:**

| Parameter | Type   | Required | Description                    |
| --------- | ------ | -------- | ------------------------------ |
| site_id   | string | Yes      | ID c·ªßa task c·∫ßn modify         |

**Request Body:**

T·∫•t c·∫£ c√°c field ƒë·ªÅu **optional** - ch·ªâ g·ª≠i field mu·ªën c·∫≠p nh·∫≠t:

```json
{
  "title": "Updated Crawl Example"
}
```

Ho·∫∑c update nhi·ªÅu field:

```json
{
  "link_type": "URL",
  "title": "Updated Crawl Example",
  "crawl_link": "https://updated-example.com"
}
```

**Request Fields:**

| Field       | Type   | Required | Description                    | Values      |
| ----------- | ------ | -------- | ------------------------------ | ----------- |
| link_type   | string | No       | Lo·∫°i link c·∫ßn crawl            | "URL", "API"|
| title       | string | No       | Ti√™u ƒë·ªÅ m·ªõi (max 255 chars)    |             |
| crawl_link  | string | No       | URL ho·∫∑c API endpoint m·ªõi      |             |

**Success Response (200 OK):**

```json
{
  "success": true,
  "task_id": "1734512345678901234",
  "message": "Crawl task modified successfully",
  "data": {
    "id": "1734512345678901234",
    "link_type": "URL",
    "title": "Updated Crawl Example",
    "crawl_link": "https://updated-example.com",
    "slug": "updated-crawl-example-nm4l8y",
    "status": "INIT",
    "created_at": "2025-12-28T11:30:00.000Z"
  }
}
```

**Error Response (400 Bad Request):**

```json
{
  "success": false,
  "error": "Bad Request",
  "message": "At least one field (link_type, title, or crawl_link) must be provided",
  "statusCode": 400
}
```

**Error Response (404 Not Found):**

```json
{
  "success": false,
  "error": "Not Found",
  "message": "Crawl task not found.",
  "statusCode": 404
}
```

**Error Response (409 Conflict):**

```json
{
  "success": false,
  "error": "Conflict",
  "message": "Slug already exists. Please use a different title.",
  "statusCode": 409
}
```

**L∆∞u √Ω quan tr·ªçng:**
- API n√†y h·ªó tr·ª£ **PARTIAL UPDATE** - ch·ªâ g·ª≠i field mu·ªën thay ƒë·ªïi
- **GI·ªÆ NGUY√äN ID** c·ªßa task, kh√¥ng t·∫°o ID m·ªõi
- Status s·∫Ω ƒë∆∞·ª£c reset v·ªÅ **INIT** khi c√≥ b·∫•t k·ª≥ field n√†o ƒë∆∞·ª£c update
- Categories v√† sub_categories s·∫Ω ƒë∆∞·ª£c reset v·ªÅ **null**
- N·∫øu update `title`, slug m·ªõi s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông sinh
- Ph·∫£i g·ª≠i √≠t nh·∫•t 1 field ƒë·ªÉ update

---

### 6. Purge Crawl Task

X√≥a vƒ©nh vi·ªÖn m·ªôt task crawl kh·ªèi h·ªá th·ªëng. API n√†y s·∫Ω x√≥a to√†n b·ªô d·ªØ li·ªáu li√™n quan.

**Endpoint:** `DELETE /api/purge-crawl/:site_id`

**URL Parameters:**

| Parameter | Type   | Required | Description                    |
| --------- | ------ | -------- | ------------------------------ |
| site_id   | string | Yes      | ID c·ªßa task c·∫ßn x√≥a            |

**Success Response (200 OK):**

```json
{
  "success": true,
  "deleted_id": "1734512345678901234"
}
```

**Error Response (404 Not Found):**

```json
{
  "success": false,
  "error": "Not Found",
  "message": "Crawl task not found.",
  "statusCode": 404
}
```

**L∆∞u √Ω quan tr·ªçng:**
- API n√†y **X√ìA Vƒ®NH VI·ªÑN** task kh·ªèi database
- T·∫•t c·∫£ d·ªØ li·ªáu li√™n quan (categories, sub_categories) s·∫Ω b·ªã x√≥a theo
- Kh√¥ng th·ªÉ kh√¥i ph·ª•c sau khi x√≥a
- S·ª≠ d·ª•ng th·∫≠n tr·ªçng trong production

---

## üîÑ Lu·ªìng x·ª≠ l√Ω

### Initialize Crawl API Flow

```
1. CLIENT g·ª≠i POST request ƒë·∫øn /api/initialize-crawl
   ‚Üì
2. VALIDATOR ki·ªÉm tra input
   - link_type ph·∫£i l√† "URL" ho·∫∑c "API"
   - title kh√¥ng ƒë∆∞·ª£c r·ªóng, max 255 chars
   - crawl_link kh√¥ng ƒë∆∞·ª£c r·ªóng
   ‚Üì
3. CONTROLLER nh·∫≠n request
   - Extract data t·ª´ request body
   - G·ªçi Service.initializeCrawl()
   ‚Üì
4. SERVICE x·ª≠ l√Ω business logic
   - Validate d·ªØ li·ªáu chi ti·∫øt
   - Generate Snowflake ID (unique distributed ID)
   - Generate slug t·ª´ title (URL-friendly)
   - Retry n·∫øu slug b·ªã tr√πng (max 5 l·∫ßn)
   - Validate URL/API format
   ‚Üì
5. REPOSITORY l∆∞u v√†o database
   - INSERT record v·ªõi status = INIT
   - categories = NULL (s·∫Ω crawl sau)
   - sub_categories = NULL (s·∫Ω crawl sau)
   ‚Üì
6. CONTROLLER format response
   - Tr·∫£ v·ªÅ task_id, slug, status
   - HTTP 201 Created
   ‚Üì
7. CLIENT nh·∫≠n response v·ªõi task_id
```

### Data Flow Example

**Input:**

```json
{
  "link_type": "URL",
  "title": "Example Website",
  "crawl_link": "https://example.com"
}
```

**Processing:**

1. Snowflake ID: `1734512345678901234`
2. Slug generation: `"Example Website"` ‚Üí `"example-website-lm3k9x"`
3. Status: `INIT`
4. Categories: `null` (s·∫Ω ƒë∆∞·ª£c populate b·ªüi API kh√°c)

**Output:**

```json
{
  "task_id": "1734512345678901234",
  "slug": "example-website-lm3k9x",
  "status": "INIT"
}
```

---

### Modify Crawl API Flow

```
1. CLIENT g·ª≠i PATCH request ƒë·∫øn /api/modify-crawl/:site_id
   ‚Üì
2. VALIDATOR ki·ªÉm tra input
   - site_id ph·∫£i t·ªìn t·∫°i trong URL params
   - √çt nh·∫•t 1 field (link_type, title, ho·∫∑c crawl_link) ph·∫£i ƒë∆∞·ª£c g·ª≠i
   - N·∫øu link_type ƒë∆∞·ª£c g·ª≠i: ph·∫£i l√† "URL" ho·∫∑c "API"
   - N·∫øu title ƒë∆∞·ª£c g·ª≠i: kh√¥ng r·ªóng, max 255 chars
   - N·∫øu crawl_link ƒë∆∞·ª£c g·ª≠i: kh√¥ng r·ªóng
   ‚Üì
3. CONTROLLER nh·∫≠n request
   - Extract site_id t·ª´ URL params
   - Extract c√°c field t·ª´ request body (ch·ªâ field c√≥ gi√° tr·ªã)
   - G·ªçi Service.modifyCrawl()
   ‚Üì
4. SERVICE x·ª≠ l√Ω business logic
   - Ki·ªÉm tra record c≈© c√≥ t·ªìn t·∫°i kh√¥ng
   - N·∫øu title ƒë∆∞·ª£c update: Generate slug m·ªõi
   - Retry n·∫øu slug b·ªã tr√πng (max 5 l·∫ßn)
   - Check slug uniqueness (tr·ª´ b·∫£n ghi ƒëang update)
   - Merge c√°c field m·ªõi v·ªõi d·ªØ li·ªáu c≈©
   - Validate URL/API format n·∫øu crawl_link ƒë∆∞·ª£c update
   ‚Üì
5. REPOSITORY update database
   - UPDATE ch·ªâ c√°c field ƒë∆∞·ª£c g·ª≠i l√™n
   - GI·ªÆ NGUY√äN ID v√† c√°c field kh√¥ng thay ƒë·ªïi
   - Reset status = INIT
   - Reset categories = NULL
   - Reset sub_categories = NULL
   ‚Üì
6. CONTROLLER format response
   - Tr·∫£ v·ªÅ task_id (gi·ªØ nguy√™n), d·ªØ li·ªáu ƒë√£ update, status
   - HTTP 200 OK
   ‚Üì
7. CLIENT nh·∫≠n response v·ªõi c√πng task_id
```

### Modify Crawl Data Flow Examples

**Example 1: Update ch·ªâ title**

**Input:**

```json
{
  "site_id": "1734512345678901234",
  "title": "New Title Only"
}
```

**Processing:**

1. Snowflake ID: `1734512345678901234` (GI·ªÆ NGUY√äN)
2. New slug: `"New Title Only"` ‚Üí `"new-title-only-ab3c5x"`
3. link_type: Gi·ªØ nguy√™n t·ª´ DB
4. crawl_link: Gi·ªØ nguy√™n t·ª´ DB
5. Status: Reset to `INIT`
6. Categories: Reset to `null`

**Output:**

```json
{
  "task_id": "1734512345678901234",
  "slug": "new-title-only-ab3c5x",
  "status": "INIT",
  "link_type": "URL",
  "crawl_link": "https://old-link.com"
}
```

**Example 2: Update ch·ªâ link_type**

**Input:**

```json
{
  "site_id": "1734512345678901234",
  "link_type": "API"
}
```

**Processing:**

1. Snowflake ID: `1734512345678901234` (GI·ªÆ NGUY√äN)
2. link_type: Thay ƒë·ªïi t·ª´ "URL" ‚Üí "API"
3. title: Gi·ªØ nguy√™n
4. slug: Gi·ªØ nguy√™n
5. crawl_link: Gi·ªØ nguy√™n
6. Status: Reset to `INIT`

**Output:**

```json
{
  "task_id": "1734512345678901234",
  "link_type": "API",
  "status": "INIT"
}
```

**Example 3: Update t·∫•t c·∫£ fields**

**Input:**

```json
{
  "site_id": "1734512345678901234",
  "link_type": "API",
  "title": "Updated API Crawler",
  "crawl_link": "https://api.new-example.com/v2"
}
```

**Processing:**

1. Snowflake ID: `1734512345678901234` (GI·ªÆ NGUY√äN)
2. New slug: `"Updated API Crawler"` ‚Üí `"updated-api-crawler-nm4l8y"`
3. link_type: "API"
4. crawl_link: "https://api.new-example.com/v2"
5. Status: Reset to `INIT`
6. Categories: Reset to `null`

**Output:**

```json
{
  "task_id": "1734512345678901234",
  "link_type": "API",
  "title": "Updated API Crawler",
  "crawl_link": "https://api.new-example.com/v2",
  "slug": "updated-api-crawler-nm4l8y",
  "status": "INIT"
}
```

---

### Purge Crawl API Flow

```
1. CLIENT g·ª≠i DELETE request ƒë·∫øn /api/purge-crawl/:site_id
   ‚Üì
2. CONTROLLER nh·∫≠n request
   - Extract site_id t·ª´ URL params
   - G·ªçi Service.purgeCrawl()
   ‚Üì
3. SERVICE x·ª≠ l√Ω business logic
   - Ki·ªÉm tra record c√≥ t·ªìn t·∫°i kh√¥ng
   - N·∫øu kh√¥ng t·ªìn t·∫°i ‚Üí throw Error (404)
   ‚Üì
4. REPOSITORY x√≥a kh·ªèi database
   - DELETE record t·ª´ crawl_site table
   - Categories v√† sub_categories (JSON fields) t·ª± ƒë·ªông b·ªã x√≥a theo
   ‚Üì
5. CONTROLLER format response
   - Tr·∫£ v·ªÅ deleted_id
   - HTTP 200 OK
   ‚Üì
6. CLIENT nh·∫≠n confirmation v·ªõi deleted_id
```

### Purge Crawl Data Flow Example

**Input:**

```json
{
  "site_id": "1734512345678901234"
}
```

**Processing:**

1. Check existence: Record found ‚úì
2. Delete operation: Remove from database
3. All data removed: ID, title, link, slug, status, categories, sub_categories

**Output:**

```json
{
  "success": true,
  "deleted_id": "1734512345678901234"
}
```

---

## 7. Get Sites List

Get list of sites with optional filtering and pagination.

### üìã Endpoint

```
GET /api/sites
```

### üì• Query Parameters

| Parameter | Type   | Required | Default | Description                                    |
| --------- | ------ | -------- | ------- | ---------------------------------------------- |
| page      | number | No       | 1       | Page number (1-indexed)                        |
| limit     | number | No       | 10      | Items per page (min: 1, max: 100)              |
| filter    | string | No       | -       | Filter by link_type ('URL' or 'API')           |

### ‚úÖ Success Response (200 OK)

```json
{
  "data": [
    {
      "id": "1734512345678901234",
      "title": "Example Website",
      "slug": "example-website",
      "link_type": "URL",
      "status": "INIT",
      "created_at": "2024-01-15T10:30:00.000Z"
    },
    {
      "id": "1734512345678901235",
      "title": "API Endpoint",
      "slug": "api-endpoint",
      "link_type": "API",
      "status": "DONE",
      "created_at": "2024-01-15T09:15:00.000Z"
    }
  ],
  "pagination": {
    "page": 1,
    "limit": 10,
    "total": 25
  }
}
```

### ‚ùå Error Responses

**Invalid Filter Value (400 Bad Request)**

```json
{
  "error": "Invalid filter value. Must be either \"URL\" or \"API\"."
}
```

### üì° cURL Examples

**Get all sites (default pagination)**

```bash
curl http://localhost:3000/api/sites
```

**Get sites with custom pagination**

```bash
curl "http://localhost:3000/api/sites?page=2&limit=20"
```

**Filter by URL type**

```bash
curl "http://localhost:3000/api/sites?filter=URL"
```

**Filter by API type**

```bash
curl "http://localhost:3000/api/sites?filter=API"
```

**Combined: Filter + Pagination**

```bash
curl "http://localhost:3000/api/sites?filter=URL&page=1&limit=5"
```

### Get Sites API Flow

```
1. CLIENT g·ª≠i GET request ƒë·∫øn /api/sites v·ªõi query params
   ‚Üì
2. CONTROLLER nh·∫≠n request
   - Parse query params: page, limit, filter
   - Convert to correct types (parseInt)
   - G·ªçi Service.getSites()
   ‚Üì
3. SERVICE x·ª≠ l√Ω business logic
   - Validate pagination (page >= 1, limit 1-100)
   - Validate filter n·∫øu c√≥ ('URL' ho·∫∑c 'API')
   - Calculate offset: (page - 1) * limit
   ‚Üì
4. REPOSITORY query database
   - Build dynamic SQL v·ªõi WHERE clause n·∫øu c√≥ filter
   - Execute COUNT query ƒë·ªÉ l·∫•y total
   - Execute SELECT query v·ªõi LIMIT/OFFSET
   - ORDER BY created_at DESC
   ‚Üì
5. SERVICE transform data
   - Map CrawlSiteModel to ISiteListItem
   - Ch·ªâ l·∫•y fields c·∫ßn thi·∫øt: id, title, slug, link_type, status, created_at
   - Format response v·ªõi pagination metadata
   ‚Üì
6. CONTROLLER tr·∫£ v·ªÅ response
   - HTTP 200 OK
   ‚Üì
7. CLIENT nh·∫≠n data array + pagination info
```

### Get Sites Data Flow Example

**Input (All Sites):**

```
GET /api/sites?page=1&limit=10
```

**Processing:**

1. Parse params: page=1, limit=10, filter=undefined
2. Normalize: page=1, limit=10, offset=0
3. Repository: SELECT * FROM crawl_site ORDER BY created_at DESC LIMIT 10 OFFSET 0
4. Count query: SELECT COUNT(*) FROM crawl_site
5. Transform to ISiteListItem[] (only essential fields)

**Output:**

```json
{
  "data": [
    {
      "id": "1734512345678901234",
      "title": "Example Site",
      "slug": "example-site",
      "link_type": "URL",
      "status": "INIT",
      "created_at": "2024-01-15T10:30:00.000Z"
    }
  ],
  "pagination": {
    "page": 1,
    "limit": 10,
    "total": 1
  }
}
```

**Input (Filtered by URL):**

```
GET /api/sites?filter=URL&page=1&limit=5
```

**Processing:**

1. Parse params: page=1, limit=5, filter='URL'
2. Validate filter: 'URL' is valid LinkType ‚úì
3. Repository: SELECT * FROM crawl_site WHERE link_type = 'URL' ORDER BY created_at DESC LIMIT 5 OFFSET 0
4. Count query: SELECT COUNT(*) FROM crawl_site WHERE link_type = 'URL'

**Output:**

```json
{
  "data": [
    {
      "id": "1734512345678901234",
      "title": "Website Crawler",
      "slug": "website-crawler",
      "link_type": "URL",
      "status": "DONE",
      "created_at": "2024-01-15T10:30:00.000Z"
    }
  ],
  "pagination": {
    "page": 1,
    "limit": 5,
    "total": 15
  }
}
```

---

## 8. Get Site Detail by Slug

Get complete site information including categories and subcategories.

### üìã Endpoint

```
GET /api/site/:slug
```

### üì• URL Parameters

| Parameter | Type   | Required | Description                    |
| --------- | ------ | -------- | ------------------------------ |
| slug      | string | Yes      | Site slug (URL-friendly ID)    |

### ‚úÖ Success Response (200 OK)

**With categories and subcategories:**

```json
{
  "id": "1734512345678901234",
  "title": "Example Website",
  "slug": "example-website-abc123",
  "link_type": "URL",
  "status": "DONE",
  "categories": [
    {
      "id": "1",
      "name": "Technology",
      "subcategories": [
        {
          "id": "1",
          "name": "Web Development"
        },
        {
          "id": "2",
          "name": "Mobile Development"
        }
      ]
    },
    {
      "id": "2",
      "name": "Business",
      "subcategories": [
        {
          "id": "3",
          "name": "Marketing"
        },
        {
          "id": "4",
          "name": "Finance"
        }
      ]
    }
  ]
}
```

**Without categories (empty array):**

```json
{
  "id": "1734512345678901234",
  "title": "New Site",
  "slug": "new-site-xyz789",
  "link_type": "API",
  "status": "INIT",
  "categories": []
}
```

### ‚ùå Error Responses

**Site Not Found (404 Not Found)**

```json
{
  "success": false,
  "error": "Not Found",
  "message": "Site not found",
  "statusCode": 404
}
```

### üì° cURL Examples

**Get site by slug**

```bash
curl http://localhost:3000/api/site/example-website-abc123
```

**Example with pretty print**

```bash
curl http://localhost:3000/api/site/example-website-abc123 | jq
```

### Get Site Detail API Flow

```
1. CLIENT g·ª≠i GET request ƒë·∫øn /api/site/:slug
   ‚Üì
2. CONTROLLER nh·∫≠n request
   - Extract slug t·ª´ URL params
   - G·ªçi Service.getSiteDetailBySlug()
   ‚Üì
3. SERVICE x·ª≠ l√Ω business logic
   - Find site by slug t·ª´ CrawlSiteRepository
   - N·∫øu kh√¥ng t√¨m th·∫•y ‚Üí throw Error (404)
   ‚Üì
4. SERVICE l·∫•y categories
   - Get categories by site_id t·ª´ CategoryRepository
   - N·∫øu kh√¥ng c√≥ categories ‚Üí return site v·ªõi categories = []
   ‚Üì
5. SERVICE l·∫•y subcategories
   - Get subcategories cho t·∫•t c·∫£ categories (batch query)
   - Map subcategories theo category_id
   ‚Üì
6. SERVICE transform data
   - Map CategoryModel[] to ICategoryDTO[]
   - Map SubcategoryModel[] to ISubcategoryDTO[]
   - Nested structure: categories[].subcategories[]
   ‚Üì
7. CONTROLLER tr·∫£ v·ªÅ response
   - HTTP 200 OK
   ‚Üì
8. CLIENT nh·∫≠n complete site detail
```

### Get Site Detail Data Flow Example

**Input:**

```
GET /api/site/tech-blog-xyz123
```

**Processing:**

1. Find site: `SELECT * FROM crawl_site WHERE slug = 'tech-blog-xyz123'`
   - Found: id=1734512345678901234, title="Tech Blog", status="DONE"

2. Get categories: `SELECT * FROM category WHERE site_id = '1734512345678901234'`
   - Found: [
       {id: 1, name: "Programming"},
       {id: 2, name: "DevOps"}
     ]

3. Get subcategories: `SELECT * FROM subcategory WHERE category_id IN (1, 2)`
   - Found: [
       {id: 1, category_id: 1, name: "JavaScript"},
       {id: 2, category_id: 1, name: "Python"},
       {id: 3, category_id: 2, name: "Docker"},
       {id: 4, category_id: 2, name: "Kubernetes"}
     ]

4. Map to response structure:
   - Group subcategories by category_id
   - Transform to DTOs (convert IDs to strings)
   - Nest subcategories inside categories

**Output:**

```json
{
  "id": "1734512345678901234",
  "title": "Tech Blog",
  "slug": "tech-blog-xyz123",
  "link_type": "URL",
  "status": "DONE",
  "categories": [
    {
      "id": "1",
      "name": "Programming",
      "subcategories": [
        {"id": "1", "name": "JavaScript"},
        {"id": "2", "name": "Python"}
      ]
    },
    {
      "id": "2",
      "name": "DevOps",
      "subcategories": [
        {"id": "3", "name": "Docker"},
        {"id": "4", "name": "Kubernetes"}
      ]
    }
  ]
}
```

**Example: Site without categories**

**Input:**

```
GET /api/site/new-site-abc
```

**Processing:**

1. Find site: Found (status=INIT)
2. Get categories: No categories found
3. Return site with empty categories array

**Output:**

```json
{
  "id": "1734512345678901235",
  "title": "New Site",
  "slug": "new-site-abc",
  "link_type": "URL",
  "status": "INIT",
  "categories": []
}
```

---

## 9. Crawl and Create Category

Crawl a site to extract category information and save it to the database.

### üìã Endpoint

```
POST /api/crawl/:site_id/category
```

### üì• URL Parameters

| Parameter | Type   | Required | Description                    |
| --------- | ------ | -------- | ------------------------------ |
| site_id   | string | Yes      | Site ID (Snowflake ID)         |

### üì• Request Body

```json
{
  "title_selector": "h2.category-title",
  "link_selector": "a.category-link"
}
```

| Field           | Type   | Required | Max Length | Description                           |
| --------------- | ------ | -------- | ---------- | ------------------------------------- |
| title_selector  | string | Yes      | 500        | CSS selector for category title       |
| link_selector   | string | Yes      | 500        | CSS selector for category link        |

**Important Notes:**
- If selector matches **multiple elements**, only the **first element** will be used
- Use specific selectors (e.g., `:first-child`, `:nth-child(1)`) for better control
- Example for multiple elements: `ul.menu-nav > li:first-child > a.nav-link`

### ‚úÖ Success Response (201 Created)

```json
{
  "site_id": "1734512345678901234",
  "category": {
    "id": "1734512345678901235",
    "slug": "technology-abc123",
    "title_selector": "h2.category-title",
    "link_selector": "a.category-link"
  }
}
```

### ‚ùå Error Responses

**Site Not Found (404 Not Found)**

```json
{
  "success": false,
  "error": "Not Found",
  "message": "Site not found",
  "statusCode": 404
}
```

**Validation Error (400 Bad Request)**

```json
{
  "success": false,
  "error": "Validation Error",
  "message": "title_selector is required; link_selector is required",
  "statusCode": 400
}
```

**Crawl Failed (500 Internal Server Error)**

```json
{
  "success": false,
  "error": "Internal Server Error",
  "message": "Crawl failed: No element found for selector: h2.category-title",
  "statusCode": 500
}
```

**Duplicate Slug (500 Internal Server Error)**

```json
{
  "success": false,
  "error": "Internal Server Error",
  "message": "Failed to generate unique slug. Please try again.",
  "statusCode": 500
}
```

### üì° cURL Examples

**Create category**

```bash
curl -X POST http://localhost:3000/api/crawl/1734512345678901234/category \
  -H "Content-Type: application/json" \
  -d '{
    "title_selector": "h2.category-title",
    "link_selector": "a.category-link"
  }'
```

**Example with different selectors**

```bash
curl -X POST http://localhost:3000/api/crawl/1734512345678901234/category \
  -

**Example with multiple matching elements (uses first)**

```bash
curl -X POST http://localhost:3000/api/crawl/1734512345678901234/category \
  -H "Content-Type: application/json" \
  -d '{
    "title_selector": "ul.menu-nav > li:first-child > a.nav-link",
    "link_selector": "ul.menu-nav > li:first-child > a.nav-link[href]"
  }'
```H "Content-Type: application/json" \
  -d '{
    "title_selector": "div.cat-name",
    "link_selector": "a.cat-url"
  }'
```

### Crawl Category API Flow

```
1. CLIENT g·ª≠i POST request ƒë·∫øn /api/crawl/:site_id/category
   ‚Üì
2. VALIDATOR ki·ªÉm tra input
   - site_id ph·∫£i t·ªìn t·∫°i trong URL params
   - title_selector: required, max 500 chars
   - link_selector: required, max 500 chars
   ‚Üì
3. CONTROLLER nh·∫≠n request
   - Extract site_id t·ª´ URL params
   - Extract selectors t·ª´ request body
   - G·ªçi Service.crawlCategory()
   ‚Üì
4. SERVICE x·ª≠ l√Ω business logic
   - Validate site t·ªìn t·∫°i
   - Launch Puppeteer browser
   - Navigate to crawl_link
   - Extract category title using title_selector
   - Generate unique slug from title
   - Generate Snowflake ID cho category
   ‚Üì
5. REPOSITORY l∆∞u v√†o database
   - INSERT category v·ªõi:
     * id: Snowflake ID
     * site_id: FK to crawl_site
     * slug: unique per site
     * title_selector, link_selector
   ‚Üì
6. CONTROLLER tr·∫£ v·ªÅ response
   - HTTP 201 Created
   ‚Üì
7. CLIENT nh·∫≠n category m·ªõi t·∫°o
```

### Crawl Category Data Flow Example

**Input:**

```
POST /api/crawl/1734512345678901234/category
{
  "title_selector": "h2.category-title",
  "link_selector": "a.category-link"
}
```

**Processing:**

1. Find site: id=1734512345678901234, crawl_link="https://example.com"
2. Launch Puppeteer:
   - Navigate to https://example.com
   - Wait for page load (networkidle2)
3. Extract title:
   - Find element matching "h2.category-title"
   - Extract textContent: "Technology News"
4. Generate slug:
   - Input: "Technology News"
   - Output: "technology-news-abc123"
   - Check uniqueness for this site_id
5. Generate ID:
   - Snowflake ID: "1734512345678901235"
6. Save to database:
   - INSERT INTO categories (id, site_id, slug, title_selector, link_selector)

**Output:**

```json
{
  "site_id": "1734512345678901234",
  "category": {
    "id": "1734512345678901235",
    "slug": "technology-news-abc123",
    "title_selector": "h2.category-title",
    "link_selector": "a.category-link"
  }
}
```

**Example: Crawl Failed**

**Input:**

```
POST /api/crawl/1734512345678901234/category
{
  "title_selector": "div.not-exist",
  "link_selector": "a.link"
}
```

**Processing:**

1. Launch Puppeteer successfully
2. Navigate to site successfully
3. Try to find element with selector "div.not-exist"
4. Element not found ‚Üí throw error

**Output:**

```json
{
  "success": false,
  "error": "Internal Server Error",
  "message": "Crawl failed: No element found for selector: div.not-exist",
  "statusCode": 500
}
```

---

## üóÑÔ∏è Database Schema

### Table: `crawl_site`

| Column          | Type                              | Description                           |
| --------------- | --------------------------------- | ------------------------------------- |
| id              | VARCHAR(20) PRIMARY KEY           | Snowflake ID                          |
| link_type       | ENUM('URL', 'API')                | Lo·∫°i link                             |
| title           | VARCHAR(255)                      | Ti√™u ƒë·ªÅ task                          |
| crawl_link      | TEXT                              | URL ho·∫∑c API endpoint                 |
| slug            | VARCHAR(255) UNIQUE               | URL-friendly identifier               |
| status          | ENUM('INIT', 'RUNNING', 'DONE', 'ERROR') | Tr·∫°ng th√°i crawl          |
| categories      | JSON NULL                         | Categories (populate sau)             |
| sub_categories  | JSON NULL                         | Sub-categories (populate sau)         |
| created_at      | TIMESTAMP                         | Th·ªùi gian t·∫°o                         |
| updated_at      | TIMESTAMP                         | Th·ªùi gian c·∫≠p nh·∫≠t                    |

**Indexes:**

- `idx_slug` - T√¨m ki·∫øm theo slug
- `idx_status` - Filter theo status
- `idx_link_type` - Filter theo link type
- `idx_created_at` - S·∫Øp x·∫øp theo th·ªùi gian

### Table: `categories`

| Column          | Type                              | Description                           |
| --------------- | --------------------------------- | ------------------------------------- |
| id              | VARCHAR(20) PRIMARY KEY           | Snowflake ID                          |
| site_id         | VARCHAR(20)                       | Foreign key to crawl_site.id          |
| slug            | VARCHAR(255)                      | URL-friendly identifier               |
| title_selector  | VARCHAR(500)                      | CSS selector for title extraction     |
| link_selector   | VARCHAR(500)                      | CSS selector for link extraction      |
| created_at      | TIMESTAMP                         | Th·ªùi gian t·∫°o                         |
| updated_at      | TIMESTAMP                         | Th·ªùi gian c·∫≠p nh·∫≠t                    |

**Foreign Keys:**

- `site_id` REFERENCES `crawl_site(id)` ON DELETE CASCADE

**Unique Constraints:**

- `unique_slug_per_site` - (site_id, slug) must be unique

**Indexes:**

- `idx_site_id` - T√¨m categories theo site
- `idx_slug` - T√¨m theo slug

**Relationship:**

```
crawl_site (1) ‚îÄ‚îÄ‚îÄ (N) categories
    ‚îÇ                      ‚îÇ
   id ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ site_id
                         ‚îÇ
                         ‚Üì
               ON DELETE CASCADE
```

**Status Flow:**

```
INIT ‚Üí RUNNING ‚Üí DONE
  ‚Üì
ERROR (n·∫øu c√≥ l·ªói)
```

---

## üß™ Testing v·ªõi Postman/cURL

### Test Initialize Crawl (Success)

```bash
curl -X POST http://localhost:3000/api/initialize-crawl \
  -H "Content-Type: application/json" \
  -d '{
    "link_type": "URL",
    "title": "Test Crawler",
    "crawl_link": "https://example.com"
  }'
```

### Test v·ªõi API type

```bash
curl -X POST http://localhost:3000/api/initialize-crawl \
  -H "Content-Type: application/json" \
  -d '{
    "link_type": "API",
    "title": "Test API Crawler",
    "crawl_link": "https://api.example.com/data"
  }'
```

### Test Validation Error

```bash
curl -X POST http://localhost:3000/api/initialize-crawl \
  -H "Content-Type: application/json" \
  -d '{
    "link_type": "INVALID",
    "title": "",
    "crawl_link": ""
  }'
```

### Test Get by ID

```bash
curl http://localhost:3000/api/crawl/1734512345678901234
```

### Test Get All

```bash
curl "http://localhost:3000/api/crawls?page=1&pageSize=10"
```

### Test Modify Crawl (Success)

```bash
curl -X PATCH http://localhost:3000/api/modify-crawl/1734512345678901234 \
  -H "Content-Type: application/json" \
  -d '{
    "link_type": "URL",
    "title": "Updated Test Crawler",
    "crawl_link": "https://updated-example.com"
  }'
```

**Expected Response:**

```json
{
  "success": true,
  "task_id": "1734512345678901234",
  "message": "Crawl task modified successfully",
  "data": {
    "id": "1734512345678901234",
    "link_type": "URL",
    "title": "Updated Test Crawler",
    "crawl_link": "https://updated-example.com",
    "slug": "updated-test-crawler-nm4l8y",
    "status": "INIT",
    "created_at": "2025-12-28T11:30:00.000Z"
  }
}
```

### Test Modify - Partial Update (ch·ªâ title)

```bash
curl -X PATCH http://localhost:3000/api/modify-crawl/1734512345678901234 \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Only Update Title"
  }'
```

### Test Modify - Partial Update (ch·ªâ link_type)

```bash
curl -X PATCH http://localhost:3000/api/modify-crawl/1734512345678901234 \
  -H "Content-Type: application/json" \
  -d '{
    "link_type": "API"
  }'
```

### Test Modify v·ªõi API type

```bash
curl -X PATCH http://localhost:3000/api/modify-crawl/1734512345678901234 \
  -H "Content-Type: application/json" \
  -d '{
    "link_type": "API",
    "title": "Updated API Endpoint",
    "crawl_link": "https://api.new-service.com/v2/data"
  }'
```

### Test Modify - No Fields Provided

```bash
curl -X PATCH http://localhost:3000/api/modify-crawl/1734512345678901234 \
  -H "Content-Type: application/json" \
  -d '{}'
```

**Expected Response:**

```json
{
  "success": false,
  "error": "Validation Error",
  "message": "At least one field (link_type, title, or crawl_link) must be provided",
  "statusCode": 400
}
```

### Test Modify - Task Not Found

```bash
curl -X PATCH http://localhost:3000/api/modify-crawl/9999999999999999999 \
  -H "Content-Type: application/json" \
  -d '{
    "link_type": "URL",
    "title": "Test",
    "crawl_link": "https://example.com"
  }'
```

**Expected Response:**

```json
{
  "success": false,
  "error": "Not Found",
  "message": "Crawl task not found.",
  "statusCode": 404
}
```

### Test Modify - Validation Error

```bash
curl -X PATCH http://localhost:3000/api/modify-crawl/1734512345678901234 \
  -H "Content-Type: application/json" \
  -d '{
    "link_type": "INVALID",
    "title": ""
  }'
```

**Expected Response:**

```json
{
  "success": false,
  "error": "Validation Error",
  "message": "link_type must be either \"URL\" or \"API\"; title cannot be empty",
  "statusCode": 400
}
```

### Test Purge Crawl (Success)

```bash
curl -X DELETE http://localhost:3000/api/purge-crawl/1734512345678901234
```

**Expected Response:**

```json
{
  "success": true,
  "deleted_id": "1734512345678901234"
}
```

### Test Purge - Task Not Found

```bash
curl -X DELETE http://localhost:3000/api/purge-crawl/9999999999999999999
```

**Expected Response:**

```json
{
  "success": false,
  "error": "Not Found",
  "message": "Crawl task not found.",
  "statusCode": 404
}
```

### Test Create Category (Success)

```bash
# First, create a site
TASK_ID=$(curl -s -X POST http://localhost:3000/api/initialize-crawl \
  -H "Content-Type: application/json" \
  -d '{
    "link_type": "URL",
    "title": "Test Site",
    "crawl_link": "https://example.com"
  }' | jq -r '.task_id')

echo "Created site: $TASK_ID"

# Then, create a category for that site
curl -X POST http://localhost:3000/api/crawl/$TASK_ID/category \
  -H "Content-Type: application/json" \
  -d '{
    "title_selector": "h1",
    "link_selector": "a"
  }'
```

**Expected Response:**

```json
{
  "site_id": "1734512345678901234",
  "category": {
    "id": "1734512345678901235",
    "slug": "example-domain-abc123",
    "title_selector": "h1",
    "link_selector": "a"
  }
}
```

### Test Create Category - Site Not Found

```bash
curl -X POST http://localhost:3000/api/crawl/9999999999999999999/category \
  -H "Content-Type: application/json" \
  -d '{
    "title_selector": "h1",
    "link_selector": "a"
  }'
```

**Expected Response:**

```json
{
  "success": false,
  "error": "Not Found",
  "message": "Site not found",
  "statusCode": 404
}
```

### Test Create Category - Validation Error

```bash
curl -X POST http://localhost:3000/api/crawl/1734512345678901234/category \
  -H "Content-Type: application/json" \
  -d '{
    "title_selector": "",
    "link_selector": ""
  }'
```

**Expected Response:**

```json
{
  "success": false,
  "error": "Validation Error",
  "message": "title_selector cannot be empty; link_selector cannot be empty",
  "statusCode": 400
}
```

### Test Get Site Detail with Categories

```bash
# Get site detail by slug (should include created categories)
SLUG=$(curl -s http://localhost:3000/api/crawl/$TASK_ID | jq -r '.data.slug')
curl http://localhost:3000/api/site/$SLUG
```

**Expected Response:**

```json
{
  "id": "1734512345678901234",
  "title": "Test Site",
  "slug": "test-site-xyz789",
  "link_type": "URL",
  "status": "INIT",
  "categories": [
    {
      "id": "1734512345678901235",
      "slug": "example-domain-abc123",
      "title_selector": "h1",
      "link_selector": "a"
    }
  ]
}
```

### Test Workflow - Create Site, Add Category, View Detail

```bash
# 1. Create a site
TASK_ID=$(curl -s -X POST http://localhost:3000/api/initialize-crawl \
  -H "Content-Type: application/json" \
  -d '{
    "link_type": "URL",
    "title": "News Website",
    "crawl_link": "https://example.com"
  }' | jq -r '.task_id')

echo "Created site: $TASK_ID"

# 2. Create first category
curl -X POST http://localhost:3000/api/crawl/$TASK_ID/category \
  -H "Content-Type: application/json" \
  -d '{
    "title_selector": "h2.category",
    "link_selector": "a.cat-link"
  }'

# 3. Create second category
curl -X POST http://localhost:3000/api/crawl/$TASK_ID/category \
  -H "Content-Type: application/json" \
  -d '{
    "title_selector": "h3.section",
    "link_selector": "a.section-url"
  }'

# 4. Get site detail with all categories
SLUG=$(curl -s http://localhost:3000/api/crawl/$TASK_ID | jq -r '.data.slug')
curl http://localhost:3000/api/site/$SLUG

# 5. Delete the site (cascade delete categories)
curl -X DELETE http://localhost:3000/api/purge-crawl/$TASK_ID
```

---

```bash
# 1. Create a task
TASK_ID=$(curl -s -X POST http://localhost:3000/api/initialize-crawl \
  -H "Content-Type: application/json" \
  -d '{
    "link_type": "URL",
    "title": "Test Task",
    "crawl_link": "https://test.com"
  }' | jq -r '.task_id')

echo "Created task: $TASK_ID"

# 2. Modify only the title
curl -X PATCH http://localhost:3000/api/modify-crawl/$TASK_ID \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Modified Test Task"
  }'

# 3. Modify multiple fields
curl -X PATCH http://localhost:3000/api/modify-crawl/$TASK_ID \
  -H "Content-Type: application/json" \
  -d '{
    "link_type": "API",
    "crawl_link": "https://api.modified-test.com"
  }'

# 4. Delete the task
curl -X DELETE http://localhost:3000/api/purge-crawl/$TASK_ID
```

### Test Get Site Detail by Slug

```bash
# Get site detail (assuming you have data in DB)
curl http://localhost:3000/api/site/example-website-abc123
```

**Expected Response:**

```json
{
  "id": "1734512345678901234",
  "title": "Example Website",
  "slug": "example-website-abc123",
  "link_type": "URL",
  "status": "DONE",
  "categories": [
    {
      "id": "1",
      "name": "Technology",
      "subcategories": [
        {"id": "1", "name": "Web Development"},
        {"id": "2", "name": "Mobile Development"}
      ]
    }
  ]
}
```

### Test Complete Flow with Sample Data

```bash
# 1. Create a site
SITE_ID=$(curl -s -X POST http://localhost:3000/api/initialize-crawl \
  -H "Content-Type: application/json" \
  -d '{
    "link_type": "URL",
    "title": "Tech Blog Example",
    "crawl_link": "https://techblog.com"
  }' | jq -r '.task_id')

SLUG=$(curl -s http://localhost:3000/api/crawl/$SITE_ID | jq -r '.data.slug')

echo "Created site: $SITE_ID with slug: $SLUG"

# 2. Insert sample categories (using MySQL)
mysql -u root -p crawler_db -e "
INSERT INTO category (site_id, name) VALUES 
('$SITE_ID', 'Programming'),
('$SITE_ID', 'DevOps');

INSERT INTO subcategory (category_id, name) VALUES 
(LAST_INSERT_ID()-1, 'JavaScript'),
(LAST_INSERT_ID()-1, 'Python'),
(LAST_INSERT_ID(), 'Docker'),
(LAST_INSERT_ID(), 'Kubernetes');
"

# 3. Get site detail with categories
curl http://localhost:3000/api/site/$SLUG | jq

# 4. Cleanup
curl -X DELETE http://localhost:3000/api/purge-crawl/$SITE_ID
```

---

## üéØ Best Practices

### 1. Code Organization

‚úÖ **MVVM Pattern**: T√°ch bi·ªát concerns  
‚úÖ **Single Responsibility**: M·ªói file/class c√≥ 1 nhi·ªám v·ª•  
‚úÖ **DRY (Don't Repeat Yourself)**: T√°i s·ª≠ d·ª•ng code  

### 2. Error Handling

‚úÖ **Centralized Error Handler**: X·ª≠ l√Ω l·ªói t·∫≠p trung  
‚úÖ **Meaningful Error Messages**: Th√¥ng b√°o l·ªói r√µ r√†ng  
‚úÖ **HTTP Status Codes**: S·ª≠ d·ª•ng ƒë√∫ng status codes  

### 3. Database

‚úÖ **Connection Pooling**: T√°i s·ª≠ d·ª•ng connections  
‚úÖ **Prepared Statements**: Tr√°nh SQL injection  
‚úÖ **Indexes**: Optimize query performance  

### 4. Security

‚úÖ **Input Validation**: Validate t·∫•t c·∫£ input  
‚úÖ **Type Safety**: TypeScript cho type checking  
‚úÖ **Environment Variables**: Sensitive data trong .env  

---

## üîÆ Future Enhancements

H·ªá th·ªëng ƒë√£ ƒë∆∞·ª£c thi·∫øt k·∫ø s·∫µn ƒë·ªÉ d·ªÖ d√†ng m·ªü r·ªông:

### 1. Crawl Categories API

```typescript
// POST /api/crawl/:id/categories
// Crawl v√† l∆∞u categories, sub_categories
```

### 2. Crawl Execution API

```typescript
// POST /api/crawl/:id/execute
// Trigger Puppeteer ƒë·ªÉ th·ª±c hi·ªán crawl
// Update status: INIT ‚Üí RUNNING ‚Üí DONE/ERROR
```

### 3. Queue System

- Th√™m Redis/Bull ƒë·ªÉ queue crawl tasks
- Background workers ƒë·ªÉ process tasks
- Retry mechanism cho failed tasks

### 4. WebSocket

- Real-time progress updates
- Status notifications

### 5. Authentication & Authorization

- JWT-based authentication
- Role-based access control

---

## üìñ Snowflake ID Explained

Snowflake ID l√† thu·∫≠t to√°n sinh ID ph√¢n t√°n c·ªßa Twitter:

**Structure (64 bits):**

```
[41 bits: Timestamp] [5 bits: Datacenter] [5 bits: Worker] [12 bits: Sequence]
```

**∆Øu ƒëi·ªÉm:**

‚úÖ **Unique**: ƒê·∫£m b·∫£o unique trong h·ªá th·ªëng ph√¢n t√°n  
‚úÖ **Time-ordered**: ID tƒÉng d·∫ßn theo th·ªùi gian  
‚úÖ **High performance**: C√≥ th·ªÉ sinh h√†ng ngh√¨n ID/gi√¢y  
‚úÖ **No central coordination**: Kh√¥ng c·∫ßn server trung t√¢m  

**Example:**

```
ID: 1734512345678901234
Timestamp: 2025-12-28 10:30:45
Datacenter: 1
Worker: 1
Sequence: 789
```

---

## ü§ù Contributing

1. Fork repository
2. Create feature branch
3. Commit changes
4. Push to branch
5. Create Pull Request

---

## üìÑ License

MIT License

---

## üë®‚Äçüíª Author

Backend Architect - Senior Level

---

## üìû Support

N·∫øu c√≥ v·∫•n ƒë·ªÅ ho·∫∑c c√¢u h·ªèi, vui l√≤ng t·∫°o issue tr√™n GitHub.

---

**Happy Coding! üöÄ**
